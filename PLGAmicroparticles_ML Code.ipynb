{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/semmatoninn/CHE1147-ML/blob/main/PLGAmicroparticles_ML%20Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# I. Data Preparation"
      ],
      "metadata": {
        "id": "XjhSQS51iwfi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 1147"
      ],
      "metadata": {
        "id": "mmgQe8b5i-xM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "!pip install rdkit\n",
        "from rdkit import Chem # module chem\n",
        "from rdkit.Chem import Draw, Descriptors\n",
        "from rdkit.Chem import PandasTools # module from rdkit.Chem\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    r2_score,\n",
        "    mean_absolute_error,\n",
        "    mean_squared_error,\n",
        "    max_error\n",
        ")\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import GridSearchCV, KFold, GroupKFold, RandomizedSearchCV\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "!pip install shap\n",
        "import shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2Pp-uDbr1ss",
        "outputId": "87ee82c9-918b-4582-ea5c-e168fc31fd57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdkit\n",
            "  Downloading rdkit-2025.9.5-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rdkit) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from rdkit) (11.3.0)\n",
            "Downloading rdkit-2025.9.5-cp312-cp312-manylinux_2_28_x86_64.whl (36.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/36.7 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2025.9.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/semmatoninn/CHE1147.git\n",
        "%cd CHE1147\n",
        "import pandas as pd\n",
        "df = pd.read_excel(\"mp_dataset_initial.xlsx\")\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "yZQnoMDvYmev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "d0ErlA8dvtn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns\n"
      ],
      "metadata": {
        "id": "K8Fpukd8Zj_C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info() # 17 columns"
      ],
      "metadata": {
        "id": "u95C_3jtv4gU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "eCob_zs5hFdp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)"
      ],
      "metadata": {
        "id": "8EfLPg3wv9Rl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dropping Source link Column - No predictive value**"
      ],
      "metadata": {
        "id": "gBBDjbxCy9Pq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(columns = 'DOI')\n",
        "print(f\"After dropping columns: the number of columns are {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "iBQiNY_FxMzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Null Values**"
      ],
      "metadata": {
        "id": "vqSt8V_fA74X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"The number of nulls in df: {df.isna().sum().sum()}\")"
      ],
      "metadata": {
        "id": "0zkCZ64Z_IEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum().sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "v6-hD8p9wd07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop columns with too many missing values - 4 columns dropped\n",
        "df = df.drop(columns=['PDI', \"Polymer Mn\",\"Polymer Mw \",\"Polymer Molecular Weight\"])\n",
        "print(f\"After dropping columns: the number of columns are {df.shape[1]}\")"
      ],
      "metadata": {
        "id": "OQGIpVmSHzH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplicates"
      ],
      "metadata": {
        "id": "3PXj-37GzXt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Number of duplicated rows: {df.duplicated().sum()}\")\n",
        "df.drop_duplicates(inplace=True) # dropping duplicates"
      ],
      "metadata": {
        "id": "z8cm0IKLyV5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Zero variance columns**"
      ],
      "metadata": {
        "id": "_NorK4QUA3qv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_var_cols = df.columns[df.nunique() == 1].tolist()\n",
        "print(\"Zero-variance columns:\", zero_var_cols)\n"
      ],
      "metadata": {
        "id": "DU7m3n_nA0Pn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "id": "8K7KvbUnz3QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(11)"
      ],
      "metadata": {
        "id": "0uE8ThIGz9i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Canonicalization**"
      ],
      "metadata": {
        "id": "U37I5Uw_OHA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def canonicalize_smiles(smiles, isomeric:bool=True): # isomeric is flag/true or false, true is default\n",
        "    \"\"\"\n",
        "    Convert any SMILES to its canonical form.\n",
        "\n",
        "    Args:\n",
        "        smiles (str): Input SMILES string\n",
        "\n",
        "    Returns:\n",
        "        str: Canonical SMILES\n",
        "    \"\"\"\n",
        "    mol = Chem.MolFromSmiles(smiles) # creating mol object\n",
        "    if mol is None:\n",
        "        return None  # Invalid SMILES\n",
        "    return Chem.MolToSmiles(mol, isomericSmiles=isomeric)"
      ],
      "metadata": {
        "id": "bdWhso6DOcUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Drug canonical_smiles'] = df['Drug SMILES'].apply(canonicalize_smiles)"
      ],
      "metadata": {
        "id": "DDazxPDKOK3U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "zYMa9KhoOGrf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a column called 'molecule' with mol objects\n",
        "PandasTools.AddMoleculeColumnToFrame(df, # AddMoleculeColumnToFrame is a function inside PandasTools module\n",
        "                                     smilesCol='Drug canonical_smiles', # input column\n",
        "                                     molCol='Drug molecule' # output column - new column created\n",
        "                                     ) # creates mol object from canonical smiles column\n",
        "\n",
        "# df['molecule'] = df['canonical_smiles'].apply(Chem.MolFromSmiles) # the previous line is same as this\n",
        "df.head()"
      ],
      "metadata": {
        "id": "rVj27jKIPX6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to calculate key molecular descriptors\n",
        "def calculate_key_descriptors(mol):\n",
        "    \"\"\"Calculate key molecular descriptors for a molecule.\"\"\"\n",
        "\n",
        "    descriptors = {\n",
        "        'Drug MW': Descriptors.MolWt(mol),\n",
        "        'Drug LogP': Descriptors.MolLogP(mol), # Lipophilicity (octanol-water partition coefficient)\n",
        "        'Drug TPSA': Descriptors.TPSA(mol), # Topological Polar Surface Area (important for drug absorption)\n",
        "        # 'Drug HBD': Descriptors.NumHDonors(mol),\n",
        "        # 'Drug HBA': Descriptors.NumHAcceptors(mol)\n",
        "    }\n",
        "\n",
        "    return descriptors\n",
        "\n",
        "# Apply the function to the molecules in the DataFrame\n",
        "descriptor_data = []\n",
        "for idx, mol in df['Drug molecule'].items(): # index is row index, mol is molecule column value, molecule is mol object, it is pd series, index value pair\n",
        "    desc = calculate_key_descriptors(mol) # dictionary created for each sample molecule\n",
        "    descriptor_data.append(desc)\n",
        "\n",
        "# Create descriptor DataFrame with pd.DataFrame()\n",
        "# We use the same index as the original DataFrame to maintain row alignment\n",
        "descriptor_df = pd.DataFrame(descriptor_data, index=df.index) # new dataframe created with descriptor columns\n",
        "\n",
        "# Combine with the original molecules DataFrame\n",
        "df_with_descriptors = pd.concat([df, descriptor_df], axis=1) # axis = 1, side by side concatenation\n",
        "\n",
        "# Check the new dimensions - we should have the same number of rows but more columns\n",
        "print(f\"Dataset shape: {df_with_descriptors.shape}\")\n",
        "df_with_descriptors.head()"
      ],
      "metadata": {
        "id": "rrjWik2NPvPJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train test split"
      ],
      "metadata": {
        "id": "YBpqgxjQL4GI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns.tolist())\n"
      ],
      "metadata": {
        "id": "5bjJ5JUWyU1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode Formulation Method\n",
        "TARGET = \"Release \"\n",
        "df_encoded = pd.get_dummies(\n",
        "    df_with_descriptors,\n",
        "    columns=[\"Formulation Method\"],\n",
        "    drop_first=True\n",
        ")\n",
        "\n",
        "# Define FEATURES (exclude target + string/RDKit fields)\n",
        "exclude_cols = [\n",
        "    TARGET,\n",
        "    \"Drug\",\n",
        "    \"Drug SMILES\",\n",
        "    \"Drug canonical_smiles\",\n",
        "    \"Drug molecule\",\n",
        "    \"Formulation Index\"\n",
        "]\n",
        "\n",
        "FEATURES = [col for col in df_encoded.columns if col not in exclude_cols]\n",
        "\n",
        "# Group-aware train/test split (no leakage)\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "\n",
        "groups = df_encoded[\"Formulation Index\"]\n",
        "\n",
        "gss = GroupShuffleSplit(test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "train_idx, test_idx = next(gss.split(df_encoded, groups=groups))\n",
        "\n",
        "#  Final datasets\n",
        "X_train = df_encoded.iloc[train_idx][FEATURES]\n",
        "X_test  = df_encoded.iloc[test_idx][FEATURES]\n",
        "\n",
        "y_train = df_encoded.iloc[train_idx][TARGET]\n",
        "y_test  = df_encoded.iloc[test_idx][TARGET]\n",
        "\n",
        "# Looking at shapes\n",
        "print(\"X_train:\", X_train.shape)\n",
        "print(\"X_test:\", X_test.shape)\n",
        "print(\"y_train:\", y_train.shape)\n",
        "print(\"y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "BnxEQYNjVXZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Correlation analysis"
      ],
      "metadata": {
        "id": "JAajZdsxCnt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pearson"
      ],
      "metadata": {
        "id": "K2ZMjAAHs9S1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_corr = X_train.copy()\n",
        "df_corr[\"Release \"] = y_train.values  # Add target column\n",
        "\n",
        "# Compute correlation\n",
        "corr_train = df_corr.corr(method=\"pearson\")\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "# Mask upper triangle for cleaner plot\n",
        "mask = np.triu(np.ones_like(corr_train, dtype=bool))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_train,\n",
        "    mask=mask,\n",
        "    cmap='coolwarm',\n",
        "    annot=True,   # Set True to show numbers\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=0.4,\n",
        "    cbar_kws={'label': 'Correlation Coefficient'}\n",
        ")\n",
        "\n",
        "plt.title(\"Correlation Heatmap (X_train + Release)\", fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "s5QTMNXWoN1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Pearson correlation - linear correlations are captured\n",
        "\n",
        "Highest correlations between features are in\n",
        "\n",
        "Drug loading capacity - Initial drug to polymer ratio 0.88\n",
        "\n",
        "Drug TPSA and MW 0.87\n",
        "\n",
        "Drug Encapsulation Efficiency and Drug Loading Capacity 0.46\n",
        "\n",
        "Highest correlation with target variable release is time 0.42\n",
        "\n"
      ],
      "metadata": {
        "id": "_0nbE9hIM36y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spearman"
      ],
      "metadata": {
        "id": "vjamFQo9L7m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Compute Spearman correlation\n",
        "corr_train = df_corr.corr(method=\"spearman\")\n",
        "\n",
        "plt.figure(figsize=(14, 12))\n",
        "\n",
        "# Mask upper triangle for cleaner plot\n",
        "mask = np.triu(np.ones_like(corr_train, dtype=bool))\n",
        "\n",
        "sns.heatmap(\n",
        "    corr_train,\n",
        "    mask=mask,\n",
        "    cmap='coolwarm',\n",
        "    annot=True,   # Set True to show numbers\n",
        "    center=0,\n",
        "    square=True,\n",
        "    linewidths=0.4,\n",
        "    cbar_kws={'label': 'Correlation Coefficient'}\n",
        ")\n",
        "\n",
        "plt.title(\"Correlation Heatmap (X_train + Release)\", fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AAxZkE36pRlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Spearman correlations non-linear correlations are also captured, monotonic relationships are captured\n",
        "\n",
        "Highest correlated features are\n",
        "\n",
        "Drug loading capacity and initial drug to polymer ratio id 0.91\n",
        "\n",
        "Drug TPSA and MW correlation is 0.62\n",
        "\n",
        "Drug encapsulation efficiency and drug loading capacity correlation is 0.49\n",
        "\n",
        "Drug LogP and Solubility Enhancer Concentration correlation is 0.43\n",
        "\n",
        "Release and time correlation is 0.67\n"
      ],
      "metadata": {
        "id": "soxdda3cOQ7Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Outliers"
      ],
      "metadata": {
        "id": "IkaDRKlC7QXp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_cols = X_train.select_dtypes(include=[np.number]).columns\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(X_train[numeric_cols], orient='h')\n",
        "# plt.tight_layout()\n",
        "plt.savefig(\"outlier_detection.png\", dpi=300, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w7H57zFCq90t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## regular split\n",
        "#from sklearn.model_selection import train_test_split\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(\n",
        "#    df[FEATURES], df[TARGET], test_size=0.2, random_state=RANDOM_SEED\n",
        "#)"
      ],
      "metadata": {
        "id": "gwsPbOV0Hj8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Modelling"
      ],
      "metadata": {
        "id": "rZOXiu0g7gbG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "rMb-XDgWUI7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_regression_metrics(model, X, y_true):\n",
        "    \"\"\"\n",
        "    Computes stable regression metrics for drug-release prediction.\n",
        "    MAPE is intentionally excluded because y contains zeros - it explodes.\n",
        "    \"\"\"\n",
        "    # Predictions\n",
        "    y_pred = model.predict(X)\n",
        "\n",
        "    # Core metrics\n",
        "    r2  = r2_score(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mxe = max_error(y_true, y_pred)\n",
        "\n",
        "    return {\n",
        "        \"r2\": r2,\n",
        "        \"mse\": mse,\n",
        "        \"mae\": mae,\n",
        "        \"max_error\": mxe\n",
        "    }\n"
      ],
      "metadata": {
        "id": "tthT63tLKrHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression - Baseline"
      ],
      "metadata": {
        "id": "2-Zzwv0JI2S9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "linear_regressor = LinearRegression()\n",
        "\n",
        "# Train the model on the training data\n",
        "linear_regressor.fit(X_train,y_train)\n",
        "\n",
        "linear_regressor_results_train = get_regression_metrics(linear_regressor,X_train,y_train )\n",
        "linear_regressor_results_test = get_regression_metrics(linear_regressor, X_test,y_test )\n",
        "\n",
        "# Print the results\n",
        "print(\"Linear Regression Results - Train Set:\", linear_regressor_results_train)\n",
        "print(\"Linear Regression Results - Test Set:\", linear_regressor_results_test)"
      ],
      "metadata": {
        "id": "rtCb2k6B4lMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression with Correlation based feature selection and scaler optimization\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rKINYRUocvnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Correlation-based feature filter\n",
        "\n",
        "class CorrelationFilter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.9):\n",
        "        self.threshold = threshold\n",
        "        self.to_drop_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
        "\n",
        "        # Pearson absolute correlation\n",
        "        corr = X_df.corr().abs()\n",
        "\n",
        "        # Upper triangle (no diag)\n",
        "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "\n",
        "        # Choose features to drop\n",
        "        self.to_drop_ = [\n",
        "            column\n",
        "            for column in upper.columns\n",
        "            if any(upper[column] > self.threshold)\n",
        "        ]\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_df = X if isinstance(X, pd.DataFrame) else pd.DataFrame(X)\n",
        "        return X_df.drop(columns=self.to_drop_, errors=\"ignore\")\n",
        "\n",
        "\n",
        "#\n",
        "# Pipeline: Corr Filter , Scaler , Linear Regression\n",
        "#    (scaler will be chosen via GridSearch)\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"corr_filter\", CorrelationFilter()),\n",
        "    (\"scaler\", StandardScaler()),   # placeholder, will be overridden by param_grid\n",
        "    (\"model\", LinearRegression())\n",
        "])\n",
        "\n",
        "#  Hyperparameter grid\n",
        "#    correlation threshold\n",
        "#    scaler type: Standard, MinMax, or no scaling\n",
        "\n",
        "param_grid = {\n",
        "    \"corr_filter__threshold\": [0.75, 0.8, 0.85, 0.9, 0.95],\n",
        "    \"scaler\": [\n",
        "        StandardScaler(),\n",
        "        MinMaxScaler(),\n",
        "        \"passthrough\"   # no scaling\n",
        "    ]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"r2\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "\n",
        "#  Fit on training data\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best params:\", grid.best_params_)\n",
        "print(\"Best CV R²:\", grid.best_score_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "linear_corr_train = get_regression_metrics(best_model, X_train, y_train)\n",
        "linear_corr_test  = get_regression_metrics(best_model, X_test, y_test)\n",
        "\n",
        "print(\"Corr-Filtered + Scaler + Linear Regression - Train:\", linear_corr_train)\n",
        "print(\"Corr-Filtered + Scaler + Linear Regression - Test :\", linear_corr_test)\n",
        "\n",
        "best_filter = best_model.named_steps[\"corr_filter\"]\n",
        "print(\"Dropped features at best threshold:\", best_filter.to_drop_)\n",
        "\n",
        "print(\"Best scaler:\", best_model.named_steps[\"scaler\"])\n"
      ],
      "metadata": {
        "id": "gIk_eIQCExco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Performance increased very slightly"
      ],
      "metadata": {
        "id": "xVrwgXUZeOZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression with Forward feature selection and scaler optimization\n",
        "\n"
      ],
      "metadata": {
        "id": "HUB8fnHI3_e4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Base estimator\n",
        "base_model = LinearRegression()\n",
        "\n",
        "# Function to generate a forward selector with a given number of features\n",
        "def make_forward_selector(k):\n",
        "    return SequentialFeatureSelector(\n",
        "        estimator=base_model,\n",
        "        n_features_to_select=k,\n",
        "        direction=\"forward\",\n",
        "        scoring=\"r2\",\n",
        "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "        n_jobs=-1\n",
        "    )\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),      # placeholder (GridSearch will override)\n",
        "    (\"feature_select\", make_forward_selector(5)),  # placeholder\n",
        "    (\"model\", LinearRegression()),\n",
        "])\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    \"scaler\": [\n",
        "        StandardScaler(),\n",
        "        MinMaxScaler(),\n",
        "        \"passthrough\"              # no scaling\n",
        "    ],\n",
        "    \"feature_select\": [\n",
        "        make_forward_selector(5),\n",
        "        make_forward_selector(8),\n",
        "        make_forward_selector(10),\n",
        "        make_forward_selector(12)\n",
        "    ],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"r2\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit the full optimized pipeline\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model and parameters\n",
        "print(\"Best Params:\", grid.best_params_)\n",
        "print(\"Best CV R²:\", grid.best_score_)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "\n",
        "train_metrics = get_regression_metrics(best_model, X_train, y_train)\n",
        "test_metrics  = get_regression_metrics(best_model, X_test, y_test)\n",
        "\n",
        "print(\"\\nOptimized Forward Selection + Scaler Pipeline\")\n",
        "print(\"Train:\", train_metrics)\n",
        "print(\"Test :\", test_metrics)\n",
        "\n",
        "\n",
        "sfs = best_model.named_steps[\"feature_select\"]\n",
        "selected_features = X_train.columns[sfs.get_support()]\n",
        "\n",
        "print(\"\\nSelected features:\")\n",
        "for f in selected_features:\n",
        "    print(\"  -\", f)\n"
      ],
      "metadata": {
        "id": "vFYkNFM4GzNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did not perform better"
      ],
      "metadata": {
        "id": "rCUdmnN44VnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##KRR"
      ],
      "metadata": {
        "id": "fM6RQDNv7noH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "krr = KernelRidge(kernel='rbf') # without regularization and no scaling\n",
        "krr.fit(X_train,y_train)\n",
        "# get the metrics on the train and the test set using the get_regression_metrics functions (as above)\n",
        "krr_results_train = get_regression_metrics(krr, X_train,y_train )\n",
        "krr_results_test = get_regression_metrics(krr, X_test,y_test )\n",
        "\n",
        "\n",
        "# the results\n",
        "print(\"KRR Results - Train Set:\", krr_results_train)\n",
        "print(\"KRR Results - Test Set:\", krr_results_test)"
      ],
      "metadata": {
        "id": "KZ0A8ti27qp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "without regularization, KRR usually overfits, shown here as well"
      ],
      "metadata": {
        "id": "fORowae1HzYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_w_scaling = Pipeline(\n",
        "   [\n",
        "       ('scaling', StandardScaler()),\n",
        "       ('krr', KernelRidge(kernel=\"rbf\"))\n",
        "   ]\n",
        ")\n",
        "pipe_w_scaling.fit(X_train,y_train)\n",
        "\n",
        "pipeline_results_train = get_regression_metrics(pipe_w_scaling, X_train, y_train )\n",
        "pipeline_results_test = get_regression_metrics(pipe_w_scaling, X_test,y_test )\n",
        "\n",
        "\n",
        "# # Print the results\n",
        "print(\"Pipeline Results - Train Set:\", pipeline_results_train)\n",
        "print(\"Pipeline Results - Test Set:\", pipeline_results_test)"
      ],
      "metadata": {
        "id": "-iArbnj08Cm-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe_w_scaling.named_steps[\"krr\"].get_params())"
      ],
      "metadata": {
        "id": "jAbzn8XLoOg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mild overfitting for standardized default krr model"
      ],
      "metadata": {
        "id": "dkFdI5Cl8guY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KRR with Regularization and Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "2tOQUluZF3ww"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# possible scalers\n",
        "scalers = [StandardScaler(), MinMaxScaler(), \"passthrough\"]\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scaling', StandardScaler()),\n",
        "    ('krr', KernelRidge(kernel='rbf'))\n",
        "])\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        'scaling': scalers,\n",
        "        'krr__alpha': [0.001, 0.01, 0.1, 1],\n",
        "        'krr__gamma': np.logspace(-3, 1, 20)\n",
        "    }\n",
        "]\n",
        "\n",
        "# Grid search with 5-fold CV\n",
        "search = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "# Print best result\n",
        "print(\"Best Params:\", search.best_params_)\n",
        "print(\"Best CV R²:\", search.best_score_)\n"
      ],
      "metadata": {
        "id": "YKht2gHuC0Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.columns"
      ],
      "metadata": {
        "id": "KMsm-CExzFkW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_krr = search.best_estimator_\n",
        "best_krr.fit(X_train, y_train)\n",
        "\n",
        "train_results = get_regression_metrics(best_krr, X_train, y_train)\n",
        "test_results  = get_regression_metrics(best_krr, X_test,  y_test)\n",
        "\n",
        "print(\"Tuned KRR - Train:\", train_results)\n",
        "print(\"Tuned KRR - Test :\", test_results)\n"
      ],
      "metadata": {
        "id": "eyVH73XXAwl0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "better than linear regression models"
      ],
      "metadata": {
        "id": "KeLCRNiSh4Y0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# group aware cross validation\n",
        "\n",
        "# groups for TRAIN SET (must align with X_train)\n",
        "groups_train = df_encoded.iloc[train_idx][\"Formulation Index\"].values\n",
        "\n",
        "# Define pipeline placeholder (scaler will be set dynamically)\n",
        "pipe = Pipeline([\n",
        "    ('scaling', StandardScaler()),  # dummy; will be overridden in param_grid\n",
        "    ('krr', KernelRidge(kernel='rbf'))\n",
        "])\n",
        "\n",
        "# Define parameter grid\n",
        "param_grid = {\n",
        "    'scaling': scalers,\n",
        "    'krr__alpha': [0.001, 0.01, 0.1, 1],\n",
        "    'krr__gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "}\n",
        "\n",
        "# Group-aware CV instead of plain cv=5\n",
        "cv = GroupKFold(n_splits=5)\n",
        "\n",
        "# Grid search with group-aware CV\n",
        "search = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    cv=cv,\n",
        "    scoring='r2',\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "search.fit(X_train, y_train, groups=groups_train)\n",
        "\n",
        "print(\"Best Params:\", search.best_params_)\n",
        "print(\"Best CV R²:\", search.best_score_)\n",
        "\n",
        "best_krr = search.best_estimator_\n",
        "best_krr.fit(X_train, y_train)\n",
        "\n",
        "train_results = get_regression_metrics(best_krr, X_train, y_train)\n",
        "test_results  = get_regression_metrics(best_krr, X_test,  y_test)\n",
        "\n",
        "print(\"Tuned KRR - Train:\", train_results)\n",
        "print(\"Tuned KRR - Test :\", test_results)"
      ],
      "metadata": {
        "id": "WNVq-dRR0kMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBoost"
      ],
      "metadata": {
        "id": "8pCphzbnbQ2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# XGBoost expects floats\n",
        "X_tr = X_train.to_numpy(dtype=float)\n",
        "y_tr = y_train.to_numpy(dtype=float)\n",
        "X_te = X_test.to_numpy(dtype=float)\n",
        "y_te = y_test.to_numpy(dtype=float)\n",
        "\n",
        "\n",
        "\n",
        "xgb = XGBRegressor(\n",
        "    n_estimators=1200,        # number of trees\n",
        "    learning_rate=0.04,       # how much each tree contributes\n",
        "    # max_depth=5,              # trees depth\n",
        "    # min_child_weight=12,      # larger leaves (regularization)\n",
        "    gamma=0.6,                # split penalty\n",
        "    # subsample=0.7,            # row sampling\n",
        "    # colsample_bytree=0.6,     # feature sampling\n",
        "    reg_alpha=1.0,            # L1\n",
        "    reg_lambda=6.0,           # L2\n",
        "    objective=\"reg:squarederror\",\n",
        "    tree_method=\"hist\",\n",
        "    n_jobs=4,\n",
        "    random_state=1147,\n",
        "    verbosity=0,\n",
        "    max_depth=4,\n",
        "    min_child_weight=5,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8\n",
        ")\n",
        "\n",
        "xgb.fit(X_tr, y_tr)\n",
        "\n",
        "print(\"XGBoost Results - Train Set:\", get_regression_metrics(xgb, X_tr, y_tr))\n",
        "print(\"XGBoost Results - Test Set:\",  get_regression_metrics(xgb, X_te, y_te))"
      ],
      "metadata": {
        "id": "fmSVisApbTIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Xgboost performance shows that this is a difficult task to predict"
      ],
      "metadata": {
        "id": "S02gKzkHRcaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypertune XGBOOST"
      ],
      "metadata": {
        "id": "To__mCOU7WDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.model_selection import GroupKFold, RandomizedSearchCV  # or GridSearchCV\n",
        "# from sklearn.metrics import make_scorer, r2_score\n",
        "\n",
        "\n",
        "groups_train = df_encoded.iloc[train_idx][\"Formulation Index\"].values\n",
        "\n",
        "print(\"Unique formulation groups in train:\", np.unique(groups_train).shape[0])\n",
        "\n",
        "xgb = XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    tree_method=\"hist\",\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "param_distributions = {\n",
        "    \"n_estimators\": [300, 500, 800, 1200],\n",
        "    \"max_depth\": [3, 4, 5, 6],\n",
        "    \"learning_rate\": [0.01, 0.03, 0.05, 0.1],\n",
        "    \"subsample\": [0.6, 0.8, 1.0],\n",
        "    \"colsample_bytree\": [0.6, 0.8, 1.0],\n",
        "    \"min_child_weight\": [1, 3, 5, 10],\n",
        "    \"gamma\": [0, 0.1, 0.3, 0.5],\n",
        "    \"reg_alpha\": [0, 0.01, 0.1, 1],\n",
        "    \"reg_lambda\": [0.1, 1, 5, 10],\n",
        "}\n",
        "\n",
        "# 4) Group-aware CV\n",
        "cv = GroupKFold(n_splits=5)\n",
        "\n",
        "# 5) RandomizedSearchCV (you can swap to GridSearchCV if you prefer)\n",
        "search_xgb = RandomizedSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=50,              # adjust depending on time\n",
        "    scoring=\"r2\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2,\n",
        "    random_state=RANDOM_SEED\n",
        ")\n",
        "\n",
        "search_xgb.fit(X_train, y_train, groups=groups_train)\n",
        "\n",
        "print(\"Best XGB Params:\", search_xgb.best_params_)\n",
        "print(\"Best XGB CV R²:\", search_xgb.best_score_)\n",
        "\n",
        "best_xgb = search_xgb.best_estimator_\n",
        "\n",
        "\n",
        "xgb_train_results = get_regression_metrics(best_xgb, X_train, y_train)\n",
        "xgb_test_results  = get_regression_metrics(best_xgb, X_test,  y_test)\n",
        "\n",
        "print(\"Tuned Group-Aware XGB - Train:\", xgb_train_results)\n",
        "print(\"Tuned Group-Aware XGB - Test :\", xgb_test_results)\n"
      ],
      "metadata": {
        "id": "FLaXviYM7yDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Best model so far"
      ],
      "metadata": {
        "id": "qIO8YoC7kJxh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# III. Model Interpretation and Explainability"
      ],
      "metadata": {
        "id": "sEBRAlSUnQ_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PFI"
      ],
      "metadata": {
        "id": "N2uBdIi0kSSw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_perm_importance(model, X, y, features, n=10):\n",
        "    \"\"\"\n",
        "    Calculate permutation importance score for each feature\n",
        "    \"\"\"\n",
        "    # Calculate baseline score (without permuting any feature)\n",
        "\n",
        "    baseline_score = model.score(X, y)\n",
        "\n",
        "    importance_scores = {}\n",
        "\n",
        "    # Loop over each feature\n",
        "    for feature in features:\n",
        "        X_perm = X.copy()\n",
        "        sum_score = 0\n",
        "\n",
        "        # Repeat n times to get average importance score\n",
        "        for i in range(n):\n",
        "            # Calculate score when given feature is permuted\n",
        "            X_perm[feature] = np.random.permutation(X_perm[feature].values)\n",
        "            permuted_score = model.score(X_perm, y)\n",
        "\n",
        "            sum_score += permuted_score\n",
        "\n",
        "        # Calculate decrease in score\n",
        "        importance_score = baseline_score - (sum_score / n)\n",
        "        importance_scores[feature] = importance_score\n",
        "\n",
        "    return importance_scores"
      ],
      "metadata": {
        "id": "SK1SXp7Xk2L6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "features_no_time = [f for f in X_test.columns if \"time\" not in f.lower()]\n",
        "\n",
        "# Compute PFI for best_xgb\n",
        "\n",
        "print(\"Calculating permutation importance for best_xgb...\")\n",
        "\n",
        "xgb_perm_importance = get_perm_importance(\n",
        "    best_xgb,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    features_no_time        # time excluded\n",
        ")\n",
        "\n",
        "#  Sort and get top 10 features\n",
        "sorted_xgb_importance = sorted(\n",
        "    xgb_perm_importance.items(),\n",
        "    key=lambda item: item[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "top_10_xgb_features = [item[0] for item in sorted_xgb_importance[:10]]\n",
        "top_10_xgb_values   = [item[1] for item in sorted_xgb_importance[:10]]\n",
        "\n",
        "print(\"\\n--- Top 10 Permutation Feature Importance (best_xgb, Time Excluded) ---\")\n",
        "for f, v in zip(top_10_xgb_features, top_10_xgb_values):\n",
        "    print(f\"{f}: {v:.5f}\")\n",
        "\n",
        "\n",
        "#  Plotting top 10 PFI\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(\n",
        "    x=top_10_xgb_values,\n",
        "    y=top_10_xgb_features,\n",
        "    palette='viridis'\n",
        ")\n",
        "plt.xlabel('Mean Decrease in Score (R²)')\n",
        "plt.ylabel('Feature')\n",
        "plt.title('Top 10 Permutation Feature Importance for XGBoost (Time Excluded)')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WiZZ9ZME960C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBOOST with PFI Feature Selection"
      ],
      "metadata": {
        "id": "jNicAlxt_OV6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Sort PFI results (highest to lowest)\n",
        "sorted_importance = sorted(\n",
        "    xgb_perm_importance.items(),\n",
        "    key=lambda x: x[1],\n",
        "    reverse=True\n",
        ")\n",
        "\n",
        "# Select Top-10 from PFI\n",
        "top_10_pfi_features = [feat for feat, score in sorted_importance[:10]]\n",
        "\n",
        "# Automatically detect REAL time feature\n",
        "time_cols = [c for c in X_train.columns if \"time\" in c.lower()]\n",
        "if len(time_cols) == 0:\n",
        "    raise ValueError(\"No time column found in X_train!\")\n",
        "time_col = time_cols[0]\n",
        "\n",
        "# Add it if missing\n",
        "if time_col not in top_10_pfi_features:\n",
        "    top_10_pfi_features.append(time_col)\n",
        "\n",
        "print(\"\\nSelected features for final model (Top-10 PFI + Time):\")\n",
        "for f in top_10_pfi_features:\n",
        "    print(\"  -\", f)\n",
        "\n",
        "# Subset train/test to these features\n",
        "X_train_top10 = X_train[top_10_pfi_features].copy()\n",
        "X_test_top10  = X_test[top_10_pfi_features].copy()\n",
        "\n",
        "#Final XGBoost (using best hyperparameters)\n",
        "final_xgb_top10 = XGBRegressor(\n",
        "    objective=\"reg:squarederror\",\n",
        "    tree_method=\"hist\",\n",
        "    random_state=RANDOM_SEED,\n",
        "    n_jobs=-1,\n",
        "    subsample=0.8,\n",
        "    reg_lambda=0.1,\n",
        "    reg_alpha=0,\n",
        "    n_estimators=300,\n",
        "    min_child_weight=1,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.03,\n",
        "    gamma=0.3,\n",
        "    colsample_bytree=0.6\n",
        ")\n",
        "\n",
        "final_xgb_top10.fit(X_train_top10, y_train)\n",
        "\n",
        "# Evaluate final model\n",
        "train_top10_metrics = get_regression_metrics(final_xgb_top10, X_train_top10, y_train)\n",
        "test_top10_metrics  = get_regression_metrics(final_xgb_top10, X_test_top10,  y_test)\n",
        "\n",
        "print(\"\\nFinal XGB (Top-10 PFI + Time) - Train:\", train_top10_metrics)\n",
        "print(\"Final XGB (Top-10 PFI + Time) - Test :\", test_top10_metrics)\n"
      ],
      "metadata": {
        "id": "QLLz8bCJCA1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SHAP Analysis"
      ],
      "metadata": {
        "id": "NIfYBqnekUh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "#Original SHAP values\n",
        "explainer = shap.TreeExplainer(final_xgb_top10)\n",
        "shap_values = explainer(X_train_top10)\n",
        "shap.plots.bar(shap_values)\n"
      ],
      "metadata": {
        "id": "FQtCO8xhvn0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beeswarm Plot"
      ],
      "metadata": {
        "id": "RqEqP73Bcu3K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "\n",
        "#Original SHAP values\n",
        "explainer = shap.TreeExplainer(final_xgb_top10)\n",
        "shap_values = explainer(X_train_top10)\n",
        "\n",
        "#Find the time column and its index\n",
        "time_cols = [c for c in X_train_top10.columns if \"time\" in c.lower()]\n",
        "time_col = time_cols[0]          # assume only one time column\n",
        "time_idx = list(X_train_top10.columns).index(time_col)\n",
        "\n",
        "#Build a mask that EXCLUDES the time column\n",
        "mask = np.array([i != time_idx for i in range(len(X_train_top10.columns))])\n",
        "\n",
        "#Create a new SHAP Explanation without time\n",
        "shap_values_no_time = shap.Explanation(\n",
        "    values       = shap_values.values[:, mask],\n",
        "    base_values  = shap_values.base_values,\n",
        "    data         = shap_values.data[:, mask],\n",
        "    feature_names=[name for i, name in enumerate(shap_values.feature_names) if i != time_idx]\n",
        ")\n",
        "\n",
        "#Beeswarm plot WITHOUT time\n",
        "shap.plots.beeswarm(shap_values_no_time)\n"
      ],
      "metadata": {
        "id": "rGRR4lF5u_WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IV. Predicted vs Actual Drug release\n"
      ],
      "metadata": {
        "id": "5IKR5EeAu9WG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# TEST portion of the original dataset ---\n",
        "test_df = df_with_descriptors.iloc[test_idx].copy()\n",
        "\n",
        "# Identify formulations present in the test set\n",
        "forms_in_test = test_df[\"Formulation Index\"].unique()\n",
        "\n",
        "# Choose 4 representative formulations\n",
        "form_sizes = test_df[\"Formulation Index\"].value_counts()\n",
        "chosen_forms = form_sizes.index[:4].tolist()\n",
        "\n",
        "print(\"Selected unseen formulations:\", chosen_forms)\n",
        "\n",
        "#plot setup\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "time_col = \"Time \"       # adjust if you stripped spaces\n",
        "release_col = \"Release \" # adjust if you stripped spaces\n",
        "\n",
        "for i, form in enumerate(chosen_forms, 1):\n",
        "\n",
        "    # subset data for this formulation\n",
        "    sub_df = test_df[test_df[\"Formulation Index\"] == form].copy()\n",
        "\n",
        "    # ensure time order\n",
        "    sub_df = sub_df.sort_values(by=time_col)\n",
        "\n",
        "    # slice matching rows from X_test\n",
        "    X_sub = X_test.loc[sub_df.index, top_10_pfi_features]\n",
        "\n",
        "    # predictions\n",
        "    sub_df[\"Release_pred\"] = final_xgb_top10.predict(X_sub)\n",
        "\n",
        "    # --- 3. Plot each formulation ---\n",
        "    plt.subplot(2, 2, i)\n",
        "\n",
        "    plt.plot(sub_df[time_col], sub_df[release_col], \"o-\", label=\"Actual\")\n",
        "    plt.plot(sub_df[time_col], sub_df[\"Release_pred\"], \"s--\", label=\"Predicted\")\n",
        "\n",
        "    plt.title(f\"Formulation {form}\")\n",
        "    plt.xlabel(\"Time (days)\")\n",
        "    plt.ylabel(\"Cumulative Release (w/w)\")\n",
        "    plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EWQ0XX42XqIj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df[\"Formulation Index\"] == 123][[\"Drug\"]]"
      ],
      "metadata": {
        "id": "5rFCMVIyZ734"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}